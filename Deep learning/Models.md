# Deep learning model

Here is a list of various deep learning models, categorized by their typical applications and architectures:

### 1. **Feedforward Neural Networks (FNNs)**
- **Multi-Layer Perceptron (MLP)**
  - Basic neural network with multiple layers, used for various tasks.

### 2. **Convolutional Neural Networks (CNNs)**
- **LeNet**
  - Early CNN architecture used for digit recognition.
- **AlexNet**
  - Pioneered deep learning in computer vision.
- **VGGNet**
  - Known for its simplicity and use of very deep networks.
- **GoogLeNet (Inception)**
  - Introduced inception modules for more efficient networks.
- **ResNet**
  - Introduced residual connections to address the vanishing gradient problem.
- **DenseNet**
  - Introduced dense connections to improve feature reuse.
- **MobileNet**
  - Efficient model for mobile and embedded vision applications.
- **EfficientNet**
  - Balances network depth, width, and resolution for better performance.

### 3. **Recurrent Neural Networks (RNNs)**
- **Vanilla RNN**
  - Basic recurrent neural network.
- **Long Short-Term Memory (LSTM)**
  - Designed to capture long-term dependencies.
- **Gated Recurrent Unit (GRU)**
  - Simplified version of LSTM.
- **Bidirectional RNN**
  - Processes data in both forward and backward directions.

### 4. **Attention Mechanisms and Transformers**
- **Attention Mechanisms**
  - Allows the model to focus on important parts of the input sequence.
- **Transformer**
  - Foundation for many modern NLP models.
- **BERT (Bidirectional Encoder Representations from Transformers)**
  - Pre-trained transformer model for NLP.
- **GPT (Generative Pre-trained Transformer)**
  - Generates coherent text and continues text sequences.
- **T5 (Text-To-Text Transfer Transformer)**
  - Unifies NLP tasks as text-to-text problems.
- **Vision Transformer (ViT)**
  - Applies transformer architecture to image classification.

### 5. **Generative Models**
- **Generative Adversarial Networks (GANs)**
  - Composed of a generator and discriminator for generating realistic data.
- **Conditional GANs (cGANs)**
  - GANs conditioned on additional information.
- **CycleGAN**
  - Used for image-to-image translation without paired examples.
- **StyleGAN**
  - Generates high-quality, realistic images.

### 6. **Autoencoders**
- **Vanilla Autoencoder**
  - Used for dimensionality reduction and feature learning.
- **Variational Autoencoder (VAE)**
  - Probabilistic model for generating new data samples.
- **Denoising Autoencoder**
  - Learns to reconstruct data from corrupted input.
- **Sparse Autoencoder**
  - Forces sparsity in the hidden representation.

### 7. **Graph Neural Networks (GNNs)**
- **Graph Convolutional Network (GCN)**
  - Extends convolutional networks to graph data.
- **Graph Attention Network (GAT)**
  - Incorporates attention mechanisms in graph processing.
- **GraphSAGE**
  - Inductive learning method for large graphs.

### 8. **Hybrid and Specialized Models**
- **Deep Belief Networks (DBNs)**
  - Composed of multiple layers of restricted Boltzmann machines.
- **Deep Reinforcement Learning**
  - Combines deep learning with reinforcement learning.
- **Capsule Networks (CapsNets)**
  - Uses capsules to better model spatial hierarchies.
- **Neural Turing Machines (NTMs)**
  - Augments neural networks with external memory.
- **Self-Organizing Maps (SOMs)**
  - Used for dimensionality reduction and clustering.
